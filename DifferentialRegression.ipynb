{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/DifferentialRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hk92RcHiLru-"
   },
   "source": [
    "# Differential Regression\n",
    "\n",
    "---\n",
    "Antoine Savine, May 2021\n",
    "---\n",
    "\n",
    "\n",
    "This notebook illustrates <b> differential machine learning </b> in the simple context of linear regression. Differential ML was introduced in the October 2020 Risk article [Differential Machine Learning: the Shape of Things to Come](https://www.risk.net/cutting-edge/banking/7688441/differential-machine-learning-the-shape-of-things-to-come) by Brian Huge and Antoine Savine. Its main promise is the design of novel, more effective, ML algorithms based on the availability of <b> differential labels </b>. Classic datasets in machine learning include inputs $X\\in\\mathbb{R}^n$ and labels $Y \\in \\mathbb{R}$. Differential labels are defined as the gradients of labels $Y$ wrt inputs $X$:\n",
    "    \n",
    "        \n",
    "$$Z^{\\left(i\\right)} \\equiv \\frac{\\partial Y^{\\left(i\\right)}}{\\partial X^{\\left(i\\right)}} \\in \\mathbb{R}^n$$\n",
    "    \n",
    "Differential labels are easily and efficiently computed with algorithmic adjoint differentiation (AAD) in some situations, like simulated datasets in finance.\n",
    "\n",
    "Probably the simplest application of differential ML is a differential variant of linear regression. Recall that linear regression works with a fixed collection of basis functions of $X$:\n",
    "\n",
    "$$\\phi \\left( X \\right)\\equiv \\left[\\phi_1\\left(X\\right), ..., \\phi_K\\left(X\\right)\\right]$$\n",
    "    \n",
    "This notebook implements polynomial regression, where the basis functions are all the monomials (also called polynomial features) of the form:\n",
    "\n",
    "$$\n",
    "    \\phi_k\\left(X\\right) = \\prod_{j=1}^{n} X_j^{p_j^k} \\text{ with } \\sum_{i=j}^n p_j^k \\leq p \n",
    "$$\n",
    "\n",
    "for a given degree $p$. The method is extended to arbitrary choices of basis functions in a straightforward manner. Note that the number $K$ of basis functions grows exponentially with dimension $n$, which is generally the case with linear regression. For this reason, linear regression is ineffective in high dimension and limited in practice to dimension lower than around $10$. More sophisticated ML models like neural networks are necessary in higher dimension, unless dimension is effectively reduced by adequate data pre-processing.\n",
    "\n",
    "Linear regression approximates the target $Y$ with a linear combination of the basis functions $\\phi$:\n",
    "\n",
    "$$\n",
    "    Y \\approx \\beta \\cdot \\phi\\left(X\\right)\n",
    "$$\n",
    "\n",
    "with weights $\\beta$ found by minimization of the mean squared error: \n",
    "\n",
    "$$MSE = E\\left\\{\\left[Y-\\beta \\cdot \\phi\\left(X\\right)\\right]^2\\right\\}$$ \n",
    "\n",
    "Zeroing the gradient of the MSE wrt weights we find the well-known normal equation:\n",
    "\n",
    "$$\n",
    "    \\beta = C_{\\phi\\phi}^{-1} C_{\\phi y}\n",
    "$$\n",
    "\n",
    "where $C_{\\phi\\phi} = E\\left[\\phi\\left(X\\right) \\phi\\left(X\\right)^T\\right] \\in \\mathbb{R}^{K \\times K}$ and $C_{\\phi y} = E\\left[\\phi\\left(X\\right) y\\right] \\in \\mathbb{R}^K$ are estimated on the training set.\n",
    "\n",
    "A more stable and effective form of regression is obtained by Tikhonov regularization, which penalizes large weights to mitigate overfitting, so as to minimize the modified cost function:\n",
    "\n",
    "$$\n",
    "    \\min \\left\\{ MSE + \\alpha \\left\\Vert \\beta \\right \\Vert ^2 \\right\\}\n",
    "$$\n",
    "\n",
    "leading to the modified normal equation:\n",
    "\n",
    "$$\n",
    "    \\beta = \\left(C_{\\phi\\phi} + \\alpha I_K\\right)^{-1} C_{\\phi y}\n",
    "$$\n",
    "\n",
    "Tikhonov regularization (also called ridge regression) is known to effectively regularize linear regression but considerably depend on regularization strength $\\alpha$. Ridge regression reverts to standard regression with small $\\alpha$, hence failing to mitigate overfitting, and converges to a horizontal line with large $\\alpha$, a phenomenon known as underfitting. Because Tikhonov regularization trades variance for bias, a sweet spot must be found, generally by cross-validation. Therefore, ridge regression is not a simple, zero-cost drop-in replacement for linear regression.\n",
    "\n",
    "Given the availability of differential labels $Z$, <b> differential regression </b> minimizes a combination of value and derivatives errors:\n",
    "    \n",
    "$$\n",
    "    \\min \\left\\{ MSE + \\sum_{j=1}^n \\alpha_j   E\\left\\{\\left[Z_j-\\beta \\cdot \\phi_j\\left(X\\right)\\right]^2\\right\\} \\right\\}\n",
    "$$\n",
    "\n",
    "where $\\phi_j\\left(X\\right) = \\left[\\frac{\\partial \\phi_1\\left(X\\right)}{\\partial X_j}, ...,  \\frac{\\partial \\phi_K\\left(X\\right)}{\\partial X_j}\\right] \\in \\mathbb{R}^K$ is the vector of partial derivatives of the basis functions wrt the j-th input $X_j$, and $Z_j$ is the j-th differential label.\n",
    "\n",
    "Zeroing the gradient of the differential objective wrt weights $\\beta$, we obtain the differential normal equation:\n",
    "\n",
    "$$\n",
    "    \\beta = \\left( C_{\\phi\\phi} + \\sum_{j=1}^n \\alpha_j C_{jj}^\\phi \\right)^{-1} \\left( C_{\\phi y} + \\sum_{j=1}^n \\alpha_j C_{jy}^\\phi \\right)\n",
    "$$\n",
    "\n",
    "where $C_{jj}^\\phi = E\\left[\\phi_j\\left(X\\right) \\phi_j\\left(X\\right)^T\\right] \\in \\mathbb{R}^{K \\times K}$ and $C_{j y}^\\phi = E\\left[\\phi_j\\left(X\\right) y\\right] \\in \\mathbb{R}^K$.\n",
    "\n",
    "Similarly to ridge regression, the hyperparameters $\\alpha_j$ control the relative importance of derivatives correctness in the minimization objective. Contrarily to ridge regression, however, differential regularization does not introduce bias. It follows that it has little risk of underfitting. A reasonable default is given by:\n",
    "\n",
    "$$\n",
    "    \\alpha_j = \\frac{E\\left[Y^2\\right]}{E\\left[Z_j^2\\right]}\n",
    "$$\n",
    "\n",
    "and doesn't generally require manual tinkering or cross-validation. It follows that, in addition to being much more effective as we will see, differential regression is a simple drop-in replacement for classic regression. Its effectiveness hinges on differential labels, which may or may not be cheaply available in a given situation. In the context of computed or simulated datasets, differential labels are always efficiently computed with AAD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndQ6wgh_Rsec"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9v32SMlUsMn"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV  \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from scipy.stats import norm # cumulative normal distribution\n",
    "\n",
    "# enable full width for wide figures\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic polynomial regression is trivially implemented by piping polynomial features with linear regression in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_polynomial(degree=5):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A factory function for ridge regression is implemented in an identical manner. Conveniently, scikit-learn has a variant with cross-validation built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression with builtin cross-validation\n",
    "def make_ridge(degree=5, alpha=1.0): # alpha = regularization strength, usually denoted lambda\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree), Ridge(alpha=alpha, normalize=True))\n",
    "\n",
    "# ridge regression with builtin cross-validation\n",
    "def make_ridge_cv(degree=5, min_alpha=1e-05, max_alpha=1e02, num_alphas=100): \n",
    "    alphas = np.exp(np.linspace(np.log(min_alpha), np.log(max_alpha), num_alphas))\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree), RidgeCV(alphas=alphas, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential regression requires a custom implementation of the equations given in the introduction. The code may be hard to read at first sight because the equations are efficiently implemented with numpy's vectorized primitives. This implementation otherwise directly reflects the equations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.0e-8\n",
    "class DifferentialRegression:\n",
    "    \n",
    "    def __init__(self, degree=5, alpha=1.0):\n",
    "        self.degree = degree\n",
    "        self.polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, x, y, z):     \n",
    "        self.phi_ = self.polynomial_features.fit_transform(x)\n",
    "        self.powers_ = self.polynomial_features.powers_\n",
    "        \n",
    "        self.dphi_ = self.phi_[:, :, np.newaxis] * self.powers_[np.newaxis, :, :] / (x[:, np.newaxis, :] + eps)\n",
    "                \n",
    "        self.lamj_ = ((y ** 2).mean(axis=0) / (z ** 2).mean(axis=0)).reshape(1,1,-1)\n",
    "        self.dphiw_ = self.dphi_ * self.lamj_\n",
    "        \n",
    "        phiTphi = np.tensordot(self.dphiw_, self.dphi_, axes=([0,2],[0,2]))\n",
    "        phiTz = np.tensordot(self.dphiw_, z, axes=([0,2],[0,1])).reshape(-1,1)\n",
    "        \n",
    "        inv = np.linalg.inv(self.phi_.T @ self.phi_ + self.alpha * phiTphi)\n",
    "        self.beta_ = (inv @ (self.phi_.T @ y + self.alpha * phiTz)).reshape(-1, 1)\n",
    "        \n",
    "    def predict(self, x, predict_derivs=False):\n",
    "        phi = self.polynomial_features.transform(x)\n",
    "        y_pred = phi @ self.beta_\n",
    "        \n",
    "        if predict_derivs:\n",
    "            dphi = phi[:, :, np.newaxis] * self.powers_[np.newaxis, :, :] / (x[:, np.newaxis, :] + eps)\n",
    "            z_pred = np.tensordot(dphi, self.beta_, (1, 0)).reshape(dphi.shape[0], -1)\n",
    "            return y_pred, z_pred\n",
    "        else:\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black & Scholes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the performance of differential regression with two simulated datasets: the one-dimensional European call in the lognormal Black & Scholes model, and the multi-dimensional basket option in the Gaussian Bachelier model. In both cases, we learn the pricing function $v\\left(X\\right)$ from examples of initial states $X$ labeled by simulated payoffs $Y$ and pathwise differentials $Z$ and compare to known ground truth on independent test sets.  \n",
    "\n",
    "Note that prices are not learned from examples of prices but from examples of payoffs, whereby each training example consists in one payoff sampled with the simulation model. The computational cost of one example is therefore the cost of simulating one Monte-Carlo path, so the entire dataset is generated for a cost similar to one pricing by Monte-Carlo. The pathwise differentials are easily computed by instrumenting the path simulation with AAD. In the simple examples presented here, pathwise differentials are computed analytically.\n",
    "\n",
    "The pricing formula is only implemented for reference, so we easily measure the out-of-sample performance of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlackScholes(S, K, sigma, T):\n",
    "    d1 = (np.log(S/K) + sigma * sigma * T) / sigma / np.sqrt(T)\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * norm.cdf(d2)\n",
    "\n",
    "def BlackScholesDelta(S, K, sigma, T):\n",
    "    d1 = (np.log(S/K) + sigma * sigma * T) / sigma / np.sqrt(T)\n",
    "    return norm.cdf(d1)\n",
    "\n",
    "class BlackScholesSimulator():\n",
    "    def __init__(self, sigma, K, T, lower=10, upper=200):\n",
    "        self.sigma = sigma\n",
    "        self.sigma2 = sigma * sigma\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.sqrtT = np.sqrt(T)\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        \n",
    "    def trainSet(self, m, seed=0):\n",
    "        s0 = np.linspace(self.lower, self.upper, m)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        wT = np.random.normal(size=m)\n",
    "        sT = s0 * np.exp(-0.5 * self.sigma2 * self.T + self.sigma * self.sqrtT * wT)\n",
    "        pay = np.maximum(0, sT - self.K)\n",
    "        deriv = np.where(sT > self.K, sT / s0, 0)\n",
    "        \n",
    "        return s0.reshape(-1, 1), pay.reshape(-1, 1), deriv.reshape(-1, 1)\n",
    "    \n",
    "    def testSet(self, m):\n",
    "        s0 = np.linspace(self.lower, self.upper, m)\n",
    "        val = BlackScholes(s0, self.K, self.sigma, self.T)\n",
    "        delta = BlackScholesDelta(s0, self.K, self.sigma, self.T)\n",
    "        return s0.reshape(-1, 1), val.reshape(-1, 1), delta.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn Black & Scholes pricing by regression over 200 evenly spaced examples. We purposely train on a small dataset to highlight overfitting and dependency on noise. Readers can easily increase the size of the dataset (variable m below) to confim convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and constants\n",
    "\n",
    "sigma = 0.2\n",
    "K = 110\n",
    "T = 2\n",
    "\n",
    "m = 200       # size of train set\n",
    "M = 5000      # size of test set\n",
    "lower = 10\n",
    "upper = 200\n",
    "\n",
    "seed = np.random.randint(8192)\n",
    "print(\"using seed\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate dataset\n",
    "\n",
    "bs = BlackScholesSimulator(sigma, K, T, lower, upper)\n",
    "x_train, y_train, z_train = bs.trainSet(m, seed=seed)\n",
    "x_test, y_test, z_test = bs.testSet(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn and predict\n",
    "\n",
    "linreg = make_polynomial()\n",
    "linreg.fit(x_train, y_train)\n",
    "linpred = linreg.predict(x_test)\n",
    "\n",
    "ridgereg = make_ridge_cv()\n",
    "ridgereg.fit(x_train, y_train)\n",
    "ridgepred = ridgereg.predict(x_test)\n",
    "alpha = ridgereg['ridgecv'].alpha_\n",
    "\n",
    "diffreg = DifferentialRegression()\n",
    "diffreg.fit(x_train, y_train, z_train)\n",
    "diffpred = diffreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "def plot_one(ax, x_train, y_train, x_test, y_test, pred):\n",
    "    ax.set_xlim(0, 225)\n",
    "    ax.set_ylim(-20, 100)\n",
    "    samples, = ax.plot(x_train, y_train, 'co', markersize=5, markerfacecolor=\"white\", label=\"samples\")\n",
    "    predict, = ax.plot(x_test, pred, 'b-', label=\"predict\")\n",
    "    correct, = ax.plot(x_test, y_test, 'r-', label=\"correct\")\n",
    "    return samples, predict, correct\n",
    "\n",
    "def plot_multi(x_train, y_train, x_test, y_test, titles, preds):\n",
    "    nplots = len(preds)\n",
    "    nrows = (nplots - 1) // 3 + 1\n",
    "    ncols = min(nplots, 3)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, ncols, squeeze=False)\n",
    "    fig.set_size_inches(ncols * 5, nrows * 5)\n",
    "\n",
    "    lines = []\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < nplots:\n",
    "            samples, predict, correct = plot_one(ax, x_train, y_train, x_test, y_test, preds[i])\n",
    "            lines.extend([samples, predict, correct])\n",
    "            ax.legend()\n",
    "            ax.set_title(titles[i])\n",
    "    \n",
    "    return fig, lines\n",
    "            \n",
    "_, _ = plot_multi(x_train, y_train, x_test, y_test, \n",
    "           [\"linear regression\", \"ridge regression with alpha=%.4f\" % alpha, \"differential regression\"],\n",
    "           [linpred, ridgepred, diffpred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better appreciate overfitting and stabilization by ridge or differentials, we repeatedly learn from independently simulated datasets and animate. <b> This cell may take about 10 seconds to render the animation </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, lines = plot_multi(x_train, y_train, x_test, y_test, \n",
    "           [\"linear regression\", \"ridge regression\", \"differential regression\"],\n",
    "           [linpred, ridgepred, diffpred])\n",
    "\n",
    "def animate(i):\n",
    "    seed = np.random.randint(8192)\n",
    "    x_train, y_train, z_train = bs.trainSet(m, seed=seed)\n",
    "    \n",
    "    linreg.fit(x_train, y_train)\n",
    "    linpred = linreg.predict(x_test)\n",
    "    ridgereg.fit(x_train, y_train)\n",
    "    ridgepred = ridgereg.predict(x_test)\n",
    "    diffreg.fit(x_train, y_train, z_train)\n",
    "    diffpred = diffreg.predict(x_test)\n",
    "    \n",
    "    lines[0].set_data(x_train, y_train)\n",
    "    lines[1].set_data(x_test, linpred)\n",
    "    lines[3].set_data(x_train, y_train)\n",
    "    lines[4].set_data(x_test, ridgepred)\n",
    "    lines[6].set_data(x_train, y_train)\n",
    "    lines[7].set_data(x_test, diffpred)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=20, interval=500, blit=True)\n",
    "\n",
    "# the conversion to jshtml is what takes time, unfortunately necessary for compatibility with Colab\n",
    "HTML(anim.to_jshtml());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic regression overfits, as illustrated by the wigly shape of its predictions, which also vary significantly in response to the noise in the dataset, a phenomenon known as variance. Cross-validated Tikhonov regularization clearly stabilizes regression, in return for a bias whereby the model struggles to match the correct shape. The results of differential regression, by contrast, appear to be consistently correct (given the small dataset) with much lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachelier basket dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Black & Scholes dataset is one-dimensional, contrarily to most real-world problems in finance, where the market state that affects the price and risk of Derivatives transactions and trading books is typically high-dimensional. To illustrate differential regression in the context of an equally simple but higher dimension example, we consider basket options in the correlated Gaussian Bachelier model. The simulation of the dataset is essentially a carbon-copy of the Black & Scholes simulator. This example is also interesting because the price and risks of a basket option only depend on the initial price of the underlying basket under the Gaussian assumption, irrespective of single stocks. This is therefore a single-dimensional problem, but of course, the fitting algorithm is unaware of this fact and treats the dataset as multi-dimensional. We can, however, conveniently visualize results with two-dimensional graphs where the horizontal axis is indexed by the initial basket.\n",
    "\n",
    "First, we have Bachelier analytics that provide the correct pricing function to easily measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bachelier(S, K, sigma, T):\n",
    "    d = (S - K) / sigma / np.sqrt(T)\n",
    "    return  sigma * np.sqrt(T) * (d * norm.cdf(d) + norm.pdf(d))\n",
    "\n",
    "def BachelierDelta(S, K, sigma, T):\n",
    "    d = (S - K) / sigma / np.sqrt(T)\n",
    "    return norm.cdf(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use helper functions to generate random correlation matrices, basket weights and volatilities, so we can easily test the methods in different circumstances without dealing with real market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a random correlation matrix\n",
    "def genCorrel(n):\n",
    "    randoms = np.random.uniform(low=-1., high=1., size=(2*n, n))\n",
    "    cov = randoms.T @ randoms\n",
    "    invvols = np.diag(1. / np.sqrt(np.diagonal(cov)))\n",
    "    return np.linalg.multi_dot([invvols, cov, invvols])\n",
    "\n",
    "# generates random weights\n",
    "def genWeights(n):\n",
    "    w = np.random.uniform(size=n)\n",
    "    return w / w.sum()\n",
    "\n",
    "# generates random volatilities such that the volatility of the basket matches a given target\n",
    "def genVols(n, correl, weights, bkt_vol=20):\n",
    "    vols = np.random.uniform(size=n)\n",
    "    weighted_vols = (weights * vols).reshape(-1,1)\n",
    "    v = np.sqrt(np.linalg.multi_dot([weighted_vols.T, correl, weighted_vols]).reshape(1))\n",
    "    vols = vols * bkt_vol / v\n",
    "    return vols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a simulator very similar to Black & Scholes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BachelierSimulator():\n",
    "    \n",
    "    # None = generate randomly\n",
    "    def __init__(self, n, T, K, vols=None, correl=None, weights=None, bkt_vol=20, seed=0, lower=10, upper=200):\n",
    "        self.n = n\n",
    "        self.T = T\n",
    "        self.sqrtT = np.sqrt(T)\n",
    "        self.K = K\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        self.weights = genWeights(n) if weights is None else weights\n",
    "        self.correl = genCorrel(n) if correl is None else correl\n",
    "        self.vols = genVols(n, self.correl, self.weights, bkt_vol) if vols is None else vols\n",
    "        self.bkt_vol = bkt_vol\n",
    "        \n",
    "        vT = np.diag(self.vols) * self.sqrtT\n",
    "        self.cov = np.linalg.multi_dot([vT, self.correl, vT])\n",
    "        self.chol = np.linalg.cholesky(self.cov) \n",
    "        \n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        \n",
    "    def trainSet(self, m, seed=0):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        s0 = np.random.uniform(low=self.lower, high=self.upper, size = (m, self.n))\n",
    "        b0 = np.dot(s0, self.weights)\n",
    "        wT = np.random.normal(size=(m, self.n))\n",
    "        sT = s0 + wT @ self.chol.T\n",
    "        bT = np.dot(sT, self.weights)\n",
    "        pay = np.maximum(0, bT - self.K)\n",
    "        deriv = np.where(bT > self.K, 1, 0).reshape(-1, 1) * self.weights.reshape(1, -1)\n",
    "        \n",
    "        return s0.reshape(-1, self.n), b0.reshape(-1, 1), pay.reshape(-1, 1), deriv.reshape(-1, self.n)\n",
    "    \n",
    "    def testSet(self, m, seed=1024): # must be a different seed\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        s0 = np.random.uniform(low=self.lower, high=self.upper, size = (m, self.n))\n",
    "        b0 = np.dot(s0, self.weights).reshape((-1, 1))\n",
    "        prices = Bachelier(b0, self.K, self.bkt_vol, self.T)\n",
    "        deltas = BachelierDelta(b0, self.K, self.bkt_vol, self.T) @ self.weights.reshape((1, -1))\n",
    "        return s0, b0, prices.reshape(-1, 1), deltas.reshape(-1, self.n)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces we need to learn basket option prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension, readers are encouraged to play with different dimensions by changing the value of n\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and constants\n",
    "\n",
    "# basket volatility is fixed but the volatilities of single stocks are randomly generated\n",
    "bkt_vol = 20\n",
    "K = 110\n",
    "T = 2\n",
    "\n",
    "# we need a larger dataset in higher dimension, at least 1,000 examples\n",
    "m = 1000\n",
    "M = 5000\n",
    "lower = 10\n",
    "upper = 200\n",
    "\n",
    "# generate random correlation, weights and volatility\n",
    "seed = np.random.randint(8192)\n",
    "correl = genCorrel(n)\n",
    "weights = genWeights(n)\n",
    "vols = genVols(n, correl, weights, bkt_vol)\n",
    "\n",
    "# re-seed for dataset\n",
    "seed = np.random.randint(8192)\n",
    "print(\"using seed\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate dataset\n",
    "\n",
    "bach = BachelierSimulator(n, T, K, vols, correl, weights, bkt_vol, seed)\n",
    "x_train, bkt_train, y_train, z_train = bach.trainSet(m, seed=seed)\n",
    "x_test, bkt_test, y_test, z_test = bach.testSet(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn and predict\n",
    "\n",
    "#linreg = make_polynomial()\n",
    "#linreg.fit(x_train, y_train)\n",
    "#linpred = linreg.predict(x_test)\n",
    "\n",
    "# linear regression is numerically unstable with nearly colinear features\n",
    "# normally, this is resolved by SVD inversion in the normal equation\n",
    "# since SVD is not implemented in the linear regression of scikit-learn\n",
    "# we use a ridge regression with small alpha as a walkaround\n",
    "\n",
    "linreg = make_ridge(alpha=1.0e-04)\n",
    "linreg.fit(x_train, y_train)\n",
    "linpred = linreg.predict(x_test)\n",
    "\n",
    "ridgereg = make_ridge_cv()\n",
    "ridgereg.fit(x_train, y_train)\n",
    "ridgepred = ridgereg.predict(x_test)\n",
    "alpha = ridgereg['ridgecv'].alpha_\n",
    "\n",
    "diffreg = DifferentialRegression()\n",
    "diffreg.fit(x_train, y_train, z_train)\n",
    "diffpred = diffreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "def plot_one(ax, bkt_train, y_train, bkt_test, y_test, pred):\n",
    "    ax.set_xlim(0, 225)\n",
    "    ax.set_ylim(-20, 100)\n",
    "    samples, = ax.plot(bkt_train, y_train, 'co', markersize=5, markerfacecolor=\"white\", label=\"samples\")\n",
    "    predict, = ax.plot(bkt_test, pred, 'b.', label=\"predict\")\n",
    "    correct, = ax.plot(bkt_test, y_test, 'r.', label=\"correct\")\n",
    "    return samples, predict, correct\n",
    "\n",
    "def plot_multi(bkt_train, y_train, bkt_test, y_test, titles, preds):\n",
    "    nplots = len(preds)\n",
    "    nrows = (nplots - 1) // 3 + 1\n",
    "    ncols = min(nplots, 3)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, ncols, squeeze=False)\n",
    "    fig.set_size_inches(ncols * 5, nrows * 5)\n",
    "\n",
    "    lines = []\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < nplots:\n",
    "            samples, predict, correct = plot_one(ax, bkt_train, y_train, bkt_test, y_test, preds[i])\n",
    "            lines.extend([samples, predict, correct])\n",
    "            ax.legend()\n",
    "            ax.set_title(titles[i])\n",
    "    \n",
    "    return fig, lines\n",
    "            \n",
    "_, _ = plot_multi(bkt_train, y_train, bkt_test, y_test, \n",
    "           [\"linear regression\", \"ridge regression with alpha=%.4f\" % alpha, \"differential regression\"],\n",
    "           [linpred, ridgepred, diffpred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate (allowing about 10 seconds for rendering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, lines = plot_multi(bkt_train, y_train, bkt_test, y_test, \n",
    "           [\"linear regression\", \"ridge regression\", \"differential regression\"],\n",
    "           [linpred, ridgepred, diffpred])\n",
    "\n",
    "def animate(i):\n",
    "    seed = np.random.randint(8192)\n",
    "    \n",
    "    x_train, bkt_train, y_train, z_train = bach.trainSet(m, seed=seed)\n",
    "    \n",
    "    linreg.fit(x_train, y_train)\n",
    "    linpred = linreg.predict(x_test)\n",
    "    ridgereg.fit(x_train, y_train)\n",
    "    ridgepred = ridgereg.predict(x_test)\n",
    "    diffreg.fit(x_train, y_train, z_train)\n",
    "    diffpred = diffreg.predict(x_test)\n",
    "    \n",
    "    lines[0].set_data(bkt_train, y_train)\n",
    "    lines[1].set_data(bkt_test, linpred)\n",
    "    lines[3].set_data(bkt_train, y_train)\n",
    "    lines[4].set_data(bkt_test, ridgepred)\n",
    "    lines[6].set_data(bkt_train, y_train)\n",
    "    lines[7].set_data(bkt_test, diffpred)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "anim=FuncAnimation(fig, animate, frames=20, interval=500, blit=True)\n",
    "\n",
    "# the conversion to jshtml is what takes time, unfortunately necessary for compatibility with Colab\n",
    "HTML(anim.to_jshtml());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential regression shines in higher dimension 3 to 7. Above dimension 7 to 10, however, all forms of linear regression become very slow, and rapidly fill computer memory. Linear regression is not effective in high dimension, and differential regularization does not provide a silver bullet. It gives a sharp improvement over classic regression and well-known forms of regularization. But its applicability remains limited to dimension under about 10. \n",
    "\n",
    "In real-world applications in finance, where dimension routinely stands in the hundreds, linear regression, differential or otherwise, is only possible when coupled with effective dimension reduction. Dimension reduction is not only strictly necessary for linear regression, it also facilitates, stabilizes and speeds-up more sophisticated ML models like neural networks. It is a recommended pre-processing step in all circumstances.\n",
    "\n",
    "Classic dimension reduction algorithms like principal components analysis (PCA) are however unsafe, prone to truncate subspaces of little variation, which may still affect prices and risk in a substantial manner. By contrast, <b> differential PCA </b>, developed as a part of the differential ML ecosystem, provides a safe and effective dimension reduction, once again, leveraging differential labels. In the basket example, differential PCA univoquely identifies the underlying basket as the only relevant regression feature, safely reducing dimension to one. Differential PCA is covered in the upcoming article <b> Axes that matter: PCA with a difference </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "DifferentialRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
